---
title: "RStudio Sparklyr: Class 3"
author: "Biagio Palese"
date: "`r format(Sys.time(), '%d %B, %Y')`" 
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE, cache = F}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(error = TRUE)
```

## RStudio Sparklyr Class 3

The following sections of our book R for Data Science( [linked phrase](https://therinspark.com/index.html) ) are included in the third class:


Chapter 4: Modeling

Up to this point we learned how to scale up data analysis to large datasets using Spark. In this class, we detail the steps required to build prediction models in Spark. We explore MLlib, the component of Spark that allows you to write high-level code to perform predictive modeling on distributed data, and use data wrangling in the context of feature engineering and exploratory data analysis.

The R interface to Spark provides modeling algorithms that should be already familiar to you, but we’ll go into detail in the next classes. For instance, we’ve already used ml_linear_regression(cars, mpg ~ .), but we could run ml_logistic_regression(cars, am ~ .) just as easily.

Take a moment to look at the long list of MLlib functions included in the appendix (section 14.5) of our book; a quick glance at this list shows that Spark supports Decision Trees, Linear Regression, Logistic Regression, K-Means Clustering, Random Forests,Linear Support Vector Machines, and more.

### Checking Prerequisites
```{r}
library(sparklyr)
system("java -version")#has to be 1.8
spark_installed_versions()#You can also check which versions are installed. Has to be 2.3


```

The focus here is on predictive modeling, since Spark aims to enable machine learning as opposed to statistical inference. Machine learning is often more concerned about forecasting the future rather than inferring the process by which our data is generated, which is then used to create automated systems.

Machine learning can be categorized into *supervised learning (predictive modeling)* and *unsupervised learning*. 
In supervised learning, we try to learn a function that will map from X to Y, from a dataset of (x, y) examples. 
In unsupervised learning, we just have X and not the Y labels, so instead we try to learn something about the structure of X. 

Some practical use cases for supervised learning include forecasting tomorrow’s weather, determining whether a credit card transaction is fraudulent, and coming up with a quote for your car insurance policy. 
With unsupervised learning, examples include automated grouping of photos of individuals, segmenting customers based on their purchase history, and clustering of documents.

### Load other required packages
```{r}
library(tidyverse)
library(dbplot)
library(sparklyr)
```

The ML interface in sparklyr has been designed to minimize the cognitive effort for moving from a local, in-memory, native-R workflow to the cluster, and back. 
What we learned in class 2 also applies here—it is important to keep track of where you are performing computations and move between the cluster and your R session as appropriate.

The examples in this class utilize the OkCupid dataset. The dataset consists of user profile data from an online dating site and contains a diverse set of features, including biographical characteristics such as gender and profession, as well as free text fields related to personal interests. 
There are about 60,000 profiles in the dataset, which fits comfortably into memory on a modern laptop and wouldn’t be considered “big data”, so you can easily follow along running Spark in local mode.

You can download this dataset as follows:
```{r}
download.file(
  "https://github.com/r-spark/okcupid/raw/master/profiles.csv.zip",
  "okcupid.zip")

unzip("okcupid.zip", exdir = "data")
unlink("okcupid.zip")
```
In addition, to follow along, you will need to install a few additional packages:
```{r}
# install.packages("ggmosaic")#in case you don't have this packages installed yet
# install.packages("forcats")
# install.packages("FactoMineR")
```

To motivate the examples, we consider the following problem:

*Predict whether someone is actively working—that is, not retired, a student, or unemployed.*
Next up, we explore this dataset.

##Exploratory Data Analysis
Exploratory data analysis (EDA), in the context of predictive modeling, is the exercise of looking at excerpts and summaries of the data. The specific goals of the EDA stage are informed by the business problem, but here are some common objectives:

- Check for data quality; confirm meaning and prevalence of missing values and reconcile statistics against existing controls.
- Understand univariate relationships between variables.
- Perform an initial assessment on what variables to include and what transformations need to be done on them.


### Getting ready
```{r}
sc <- spark_connect(master = "local", version = "2.3")#opening the connection

okc <- spark_read_csv(
  sc, 
  "data/profiles.csv", 
  escape = "\"", 
  memory = FALSE,
  options = list(multiline = TRUE)
) %>%
  mutate(
    height = as.numeric(height),
    income = ifelse(income == "-1", NA, as.numeric(income))
  ) %>%
  mutate(sex = ifelse(is.na(sex), "missing", sex)) %>%
  mutate(drinks = ifelse(is.na(drinks), "missing", drinks)) %>%
  mutate(drugs = ifelse(is.na(drugs), "missing", drugs)) %>%
  mutate(job = ifelse(is.na(job), "missing", job))#reading in spark the data downloaded and perform some manipulation to its variables

#We specify escape = "\"" and options = list(multiline = TRUE) here to accommodate embedded quote characters and newlines in the essay fields. We also convert the height and income columns to numeric types and recode missing values in the string columns. Note that it might very well take a few tries of specifying different parameters to get the initial data ingest correct, and sometimes you might need to revisit this step after you learn more about the data during modeling.
```

We can now take a quick look at our data
```{r}
okc#print in console
glimpse(okc)#new function that allows you to visualize all columns and few observation in each column
```
Now, we add our response variable as a column in the dataset and look at its distribution:
```{r}
okc <- okc %>%
  mutate(
    not_working = ifelse(job %in% c("student", "unemployed", "retired"), 1 , 0)
  )#creating our variable of interest

okc %>% 
  group_by(not_working) %>% 
  count()
```
Before we proceed further, let’s perform an initial split of our data into a training set and a testing set and put away the latter. In practice, this is a crucial step because we would like to have a holdout set that we set aside at the end of the modeling process to evaluate model performance. 
If we were to include the entire dataset during EDA, information from the testing set could “leak” into the visualizations and summary statistics and bias our model-building process even though the data is not used directly in a learning algorithm. This would undermine the credibility of our performance metrics. We can easily split the data by using the sdf_random_split() function:
```{r}
data_splits <- sdf_random_split(okc, training = 0.8, testing = 0.2, seed = 1234)#by adding a seed you can recreate this split in future
okc_train <- data_splits$training
okc_test <- data_splits$testing
```
We can quickly look at the distribution of our response variable:
```{r}
okc_train %>%
  group_by(not_working) %>%
  count() %>%
  ungroup() %>% 
  mutate(frac = n / sum(n))
#Exercise: you can achieve the same results using the function tally. Lear about that function ?tally and replicate the results
okc_train %>% group_by(not_working) %>% tally() %>% mutate(fact=n / sum(n))
```
Using the sdf_describe() function, we can obtain numerical summaries of specific columns:
```{r}
sdf_describe(okc_train, cols = c("age", "income"))#new function for quick summarization of your variables
#Exercise replicate the results using summarise
okc_train %>% summarise(n(),mean(age),sd(age),min(age),max(age) )
okc_train %>% summarise(n(),mean(income),sd(income),min(income),max(income))
```
We now utilize the dbplot package to plot distributions of these variables. 
```{r}
dbplot_histogram(okc_train, age)
#Exercise plot the income variable
dbplot_histogram(okc_train,income)
```
A common EDA exercise is to look at the relationships between the response and the individual predictors. Often, you might have prior business knowledge of what these relationships should be, so this can serve as a data quality check. Also, unexpected trends can inform variable interactions that you might want to include in the model. As an example, we can explore the religion variable:
```{r}
prop_data <- okc_train %>%
  mutate(religion = regexp_extract(religion, "^\\\\w+", 0)) %>% 
  group_by(religion, not_working) %>%
  tally() %>%
  group_by(religion) %>%
  summarize(
    count = sum(n),
    prop = sum(not_working * n) / sum(n)
  ) %>%
  mutate(se = sqrt(prop * (1 - prop) / count)) %>%
  collect()# Try to run one pipe at the time and understand what is happening in the above code. 
#Where se is the standard error of a statistic is the standard deviation of its sampling distribution or an estimate of that standard deviation.

prop_data#let's look at the dataset. To see for each religion what is the proportion of people whom is not working.
```

Note that prop_data is a small DataFrame that has been collected into memory in our R session, we can take advantage of ggplot2 to create an informative visualization, no need of dbplot
```{r}
prop_data %>%
  ggplot(aes(x = religion, y = prop)) + geom_point(size = 2) +
  geom_errorbar(aes(ymin = prop - 1.96 * se, ymax = prop + 1.96 * se),
                width = .1) +
  geom_hline(yintercept = sum(prop_data$prop * prop_data$count) /
                              sum(prop_data$count))#run one geom at                               the time to understand what they are doing
```
Next, we take a look at the relationship between a couple of predictors: alcohol use and drug use. We would expect there to be some correlation between them. You can compute a contingency table via sdf_crosstab():
```{r}
contingency_tbl <- okc_train %>% 
  sdf_crosstab("drinks", "drugs") %>%
  collect()#again we perform computation in Spark and collect results back in R

contingency_tbl
```
We can visualize this contingency table using a mosaic plot:
```{r}
library(ggmosaic)


contingency_tbl %>%
  rename(drinks = drinks_drugs) %>%
  gather("drugs", "count", missing:sometimes) %>%
  mutate(
    drinks = as_factor(drinks) %>% 
      fct_relevel("missing", "not at all", "rarely", "socially", 
                  "very often", "desperately"),
    drugs = as_factor(drugs) %>%
      fct_relevel("missing", "never", "sometimes", "often")
  ) %>%
  ggplot() +
  geom_mosaic(aes(x = product(drinks, drugs), fill = drinks, 
                  weight = count))
```
To further explore the relationship between these two variables, we can perform correspondence analysis using the FactoMineR package. This technique enables us to summarize the relationship between the high-dimensional factor levels by mapping each level to a point on the plane. We first obtain the mapping using FactoMineR::CA() as follows:
```{r}
dd_obj <- contingency_tbl %>% 
  tibble::column_to_rownames(var = "drinks_drugs") %>%
  FactoMineR::CA(graph = FALSE)
```
We can then plot the results using ggplot:
```{r}
dd_drugs <-
  dd_obj$row$coord %>%
  as.data.frame() %>%
  mutate(
    label = gsub("_", " ", rownames(dd_obj$row$coord)),
    Variable = "Drugs"
  )

dd_drinks <-
  dd_obj$col$coord %>%
  as.data.frame() %>%
  mutate(
    label = gsub("_", " ", rownames(dd_obj$col$coord)),
    Variable = "Alcohol"
  )
  
ca_coord <- rbind(dd_drugs, dd_drinks)
  
ggplot(ca_coord, aes(x = `Dim 1`, y = `Dim 2`, 
                     col = Variable)) +
  geom_vline(xintercept = 0) +
  geom_hline(yintercept = 0) +
  geom_text(aes(label = label)) +
  coord_equal()
```
In the above chart, we see that the correspondence analysis procedure has transformed the factors into variables called principal coordinates, which correspond to the axes in the plot and represent how much information in the contingency table they contain. We can, for example, interpret the proximity of “drinking often” and “using drugs very often” as indicating association.

This concludes our discussion on EDA. Let’s proceed to feature engineering.

###Feature Engineering
The feature engineering exercise comprises transforming the data to increase the performance of the model. This can include things like centering and scaling numerical values and performing string manipulation to extract meaningful variables. It also often includes variable selection—the process of selecting which predictors are used in the model.
```{r}
dbplot_histogram(okc_train, age)
```

In the above chart we see that the age variable has a range from 18 to over 60. Some algorithms, especially neural networks, train faster if we normalize our inputs so that they are of the same magnitude. Let’s now normalize the age variable by removing the mean and scaling to unit variance, beginning by calculating its mean and standard deviation:
```{r}
scale_values <- okc_train %>%
  summarize(
    mean_age = mean(age),
    sd_age = sd(age)
  ) %>%
  collect()

scale_values
```
We can then use these to transform the dataset:
```{r}
okc_train <- okc_train %>%
  mutate(scaled_age = (age - !!scale_values$mean_age) /
           !!scale_values$sd_age)
# #Previously `df$x` or
# `df[[y]]` implied that `df` was a local variable, but now you
# must make that explict with `!!` or `local()`, e.g., `!!df$x` or
# `local(df[["y"]))

```
In the below chart, we see that the scaled age variable has values that are closer to zero. We now move on to discussing another type of transformation, but during your feature engineering workflow you might want to perform the normalization for all numeric variables that you want to include in the model.
```{r}
dbplot_histogram(okc_train, scaled_age)
```

For the free text fields, a straightforward way to extract features is counting the total number of characters. We will store the train dataset in Spark’s memory with compute() to speed up computation.
```{r}
okc_train <- okc_train %>%
  mutate(
    essay_length = char_length(paste(!!!syms(paste0("essay", 0:9))))
  ) %>% compute()
```
We can see the distribution of the essay_length variable
```{r}
dbplot_histogram(okc_train, essay_length, bins = 100)
```
Now that we have a few more features to work with, we can begin running an unsupervised learning algorithm.

##Clustering: Latent Dirichlet Allocation (LDA)-->	ml_lda() function
In Spark, LDA is listed among the clustering functions (appendix 14.5). This is not a text analysis class but the objective is to show how to perform a basic topic-modeling task on the essay data in the OKCupid dataset. The plan is to concatenate the essay fields (of which there are 10) of each profile and regard each profile as a document, then attempt to discover topics (we define these soon) using Latent Dirichlet Allocation (LDA). 

### Data Preparation
As always, before analyzing a dataset (or a subset of one), we want to take a look at it to orient ourselves. In this case, we are interested in the freeform text that the users entered into their dating profiles.
```{r}
essay_cols <- paste0("essay", 0:9)#combining the 10 essay fields
essays <- okc %>%
  select(!!essay_cols)
essays %>% 
  glimpse()
```
Just from this output, we see the following:

- The text contains HTML tags
- The text contains the newline (\n) character
- There are missing values in the data
The HTML tags and special characters pollute the data since they are not directly input by the user and do not provide interesting information. Similarly, since we have encoded missing character fields with the missing string, we need to remove it. (Note that by doing this we are also removing instances of the word “missing” written by the users, but the information lost from this removal is likely to be small.)

As you analyze your own text data, you will quickly come across and become familiar with the peculiarities of the specific dataset. As with tabular numerical data, preprocessing text data is an iterative process, and after a few tries we have the following transformations:
```{r}
essays <- essays %>%
  # Replace `missing` with empty string.
  mutate_all(list(~ ifelse(. == "missing", "", .))) %>%
  # Concatenate the columns.
  mutate(essay = paste(!!!syms(essay_cols))) %>%
  # Remove miscellaneous characters and HTML tags
  mutate(words = regexp_replace(essay, "\\n|&nbsp;|<[^>]*>|[^A-Za-z|']", " "))#Note here we are using regex_replace(), which is a Spark SQL function. 
```
Next, we discuss LDA and how to apply it to our cleaned dataset.

### Topic Modeling
LDA is a type of topic model for identifying abstract “topics” in a set of documents. It is an unsupervised algorithm in that we do not provide any labels, or topics, for the input documents. LDA posits that each document is a mixture of topics, and each topic is a mixture of words. During training, it attempts to estimate both of these simultaneously. A typical use case for topic models involves categorizing many documents, for which the large number of documents renders manual approaches infeasible. 
After we have a reasonably clean dataset following the workflow in the previous section, we can fit an LDA model with ml_lda():
```{r}
stop_words <- ml_default_stop_words(sc) %>%
  c(
    "like", "love", "good", "music", "friends", "people", "life",
    "time", "things", "food", "really", "also", "movies"
  )#We are also including a stop_words vector, consisting of commonly used English words and common words in our dataset, that instructs the algorithm to ignore them. 

lda_model <-  ml_lda(essays, ~ words, k = 6, max_iter = 1, min_token_length = 4, 
                     stop_words = stop_words, min_df = 5)#We run the model on only 1 iterations.  You should run hundres of iterations (by changing the number of max.iter parameter) and see how the output would change. However, beware that this can take a really long time in a single machine—this is the kind of big-compute problem that a proper Spark cluster would be able to easily tackle. So, for now let's leave only 1 iteration
```
After the model is fit, we can use the tidy() function to extract the associated betas, which are the per-topic-per-word probabilities, from the model.

```{r}
betas <- tidy(lda_model)
betas
```
We can then visualize this output by looking at word probabilities by topic. 
```{r}
betas %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  arrange(topic, -beta) %>%
  mutate(term = reorder(term, beta)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
    geom_col(show.legend = FALSE) +
    facet_wrap(~ topic, scales = "free") +
    coord_flip()
```
Using only 1 iteration do not results in interpretable topics. At 100 iterations, we can see “topics” starting to emerge. This could be interesting information in its own right if you were digging into a large collection of documents with which you aren’t familiar. 

The learned topics can also serve as features in a downstream supervised learning task; for example, we could consider using the topic number as a predictor in our model to predict employment status in our predictive modeling example. However, let's leave this analysis to one your future company will make available clusters to run your data.
We are at the end of class 3 so we go ahead and disconnect our Spark cluster.

```{r}
spark_disconnect(sc)
```
####Lesson learned

In this class, we covered the basics of building predictive models in Spark with R by presenting the topics of EDA, feature engineering, and using unsupervised learning to process raw text, in which you created a topic model that automatically grouped the profiles into six categories. 


##### Sparklyr Resources
There are many additional resources that can help you to troubleshoot particular issues while getting started and, in general, introduce you to the broader Spark and R communities to help you get specific answers, discuss topics, and connect with many users actively using Spark with R:

- Documentation
The documentation site hosted in RStudio’s Spark website [linked phrase](https://spark.rstudio.com) should be your first stop to learn more about Spark when using R. The documentation is kept up to date with examples, reference functions, and many more relevant resources.
- Blog
To keep up to date with major sparklyr announcements, you can follow the RStudio blog [linked phrase](https://blog.rstudio.com/tags/sparklyr).
- Community
For general sparklyr questions, you can post in the RStudio Community tagged as sparklyr [linked phrase](https://community.rstudio.com/tags/sparklyr).
- Stack Overflow
For general Spark questions, Stack Overflow is a great resource; there are also many topics specifically about sparklyr [linked phrase](https://stackoverflow.com/questions/tagged/sparklyr).
- Github
If you believe something needs to be fixed, open a GitHub issue or send us a pull request [linked phrase](https://github.com/sparklyr/sparklyr).
- Gitter
For urgent issues or to keep in touch, you can chat with package authors in Gitter [linked phrase](https://gitter.im/rstudio/sparklyr).
