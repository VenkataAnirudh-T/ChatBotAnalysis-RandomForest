---
title: "RStudio:  Regression"
author: "Biagio Palese"
date: "`r format(Sys.time(), '%d %B, %Y')`" 
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE, cache = F}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(error = TRUE)
```

Main source for this class materials: ( [linked phrase](http://uc-r.github.io/linear_regression) )
Recommended read:( [linked phrase](http://uc-r.github.io/regression_preparation)

##  Regression

Linear regression is a very simple approach for supervised learning. In particular, linear regression is a useful tool for predicting a quantitative response. Linear regression has been around for a long time and is the topic of innumerable textbooks. Though it may seem somewhat dull compared to some of the more modern statistical learning approaches, linear regression is still a useful and widely used statistical learning method. Moreover, it serves as a good jumping-off point for newer approaches: many fancy statistical learning approaches can be seen as generalizations or extensions of linear regression. Consequently, the importance of having a good understanding of linear regression before studying more complex learning methods cannot be overstated.

### Requirements
We will use advertising data provided by the authors of An Introduction to Statistical Learning. This is a simple data set that contains, in thousands of dollars, TV, Radio, and Newspaper budgets for 200 different markets along with the Sales, in thousands of units, for each market. We’ll also use a few packages that provide data manipulation, visualization, pipeline modeling functions, and model output tidying functions.
```{r}
library(tidyverse)
library(modelr)     # provides easy pipeline modeling                             functions
library(broom)      # helps to tidy up model outputs


advertising <- read_csv("Advertising.csv") %>%
  select(-X1)# load data (remove row numbers included as X1 variable)

advertising
```

### Data Preparation
Initial discovery of relationships is usually done with a training set while a test set is used for evaluating whether the discovered relationships hold. More formally, a training set is a set of data used to discover potentially predictive relationships. A test set is a set of data used to assess the strength and utility of a predictive relationship. 
We’ll use a conventional 60% / 40% split where we training our model on 60% of the data and then test the model performance on 40% of the data that is withheld.

```{r}
set.seed(123)
sample <- sample(c(TRUE, FALSE), nrow(advertising), replace = T, prob = c(0.6,0.4))# do you understand what this code is doing? Try ?sample
train <- advertising[sample, ]
test <- advertising[!sample, ]
```

##Simple Linear Regression
Simple linear regression lives up to its name: it is a very straightforward approach for predicting a quantitative response Y on the basis of a single predictor variable 
X. It assumes that there is approximately a linear relationship between X and Y. Using our advertising data, suppose we wish to model the linear relationship between the TV budget and sales. We can write this as:

![Equation 1](images/regression/regression1.png)
### Model Building
To build this model in R we use the formula notation of 
Y∼X.
```{r}
model1 <- lm(sales ~ TV, data = train)
?lm
```
In the background the lm, which stands for “linear model”, is producing the best-fit linear relationship by minimizing the least squares criterion. 
This fit can be visualized in the following illustration where the “best-fit” line is found by minimizing the sum of squared errors (the errors are represented by the vertical black line segments).
![Figure 1](images/regression/regression2.png)
For initial assessment of our model we can use summary. This provides us with a host of information about our model, which we’ll walk through. 
```{r}
summary(model1)
glance(model1)#Alternatively, you can also get a “tidy” result output.
?glance
```
### Assessing Coefficients
Our original formula in Eq. (1) includes 
β0 for our intercept coefficent and β1 for our slope coefficient. If we look at our model results (here we use tidy to just print out a tidy version of our coefficent results) we see that our model takes the form of:
![Equation 2](images/regression/regression3.png)

```{r}
tidy(model1)
?tidy
```
In other words, our intercept estimate is 6.76 so when the TV advertising budget is zero we can expect sales to be 6,760 (remember we’re operating in units of 1,000). And for every $1,000 increase in the TV advertising budget we expect the average increase in sales to be 50 units.

It’s also important to understand if the these coefficients are statistically significant. In other words, can we state these coefficients are statistically different then 0? To do that we can start by assessing the standard error (SE). SE represents the average distance that the observed values fall from the regression line. Conveniently, it tells you how wrong the regression model is on average using the units of the response variable. The SE for β0 and β1 are computed with:
![Equations 3 and 4](images/regression/regression4.png)

To get this information in R we can simply use:
```{r}
confint(model1)
```
Our results show us that our 95% confidence interval for 
β1 (TV) is [.043, .057]. Thus, since zero is not in this interval we can conclude that as the TV advertising budget increases by $1,000 we can expect the sales to increase by 43-57 units. This is also supported by the t-statistic provided by our results, which are computed by:
![Equation 5](images/regression/regression5.png)
which measures the number of standard deviations that 
β1 is away from 0. Thus a large t-statistic such as ours will produe a small p-value (a small p-value indicates that it is unlikely to observe such a substantial association between the predictor variable and the response due to chance). 
```{r}
summary(model1)#the stars indicates the significant levels of each variables and are determined by the p-values
```

Thus, we can conclude that a relationship between TV advertising budget and sales exists.

### Assessing Model Accuracy
Next, we want to understand the extent to which the model fits the data. This is typically referred to as the goodness-of-fit. We can measure this quantitatively by assessing three things:

  - *Residual standard error (RSE)*
  - *R squared (R2)*
  - *F-statistic*
The RSE is an estimate of the standard deviation of 
ϵ (error terms). Roughly speaking, it is the average amount that the response will deviate from the true regression line. It is computed by:
![Equation 6](images/regression/regression6.png)
We get the RSE at the bottom of summary(model1), we can also get it directly with:
```{r}
sigma(model1)
```
An RSE value of 3.2 means the actual sales in each market will deviate from the true regression line by approximately 3,200 units, on average. Is this significant? Well, that’s subjective but when compared to the average value of sales over all markets the percentage error is 22%:
```{r}
sigma(model1)/mean(train$sales)
```
The RSE provides an absolute measure of lack of fit of our model to the data. But since it is measured in the units of 
Y, our response variable, it is not always clear what constitutes a good RSE. 
The R2 statistic provides an alternative measure of fit. It represents the proportion of variance explained and so it always takes on a value between 0 and 1, and is independent of the scale of Y. R2 is simply a function of residual sum of squares (RSS) and total sum of squares (TSS):
![Equation 7](images/regression/regression7.png)
Similar to RSE the R2 can be found at the bottom of summary(model1) but we can also get it directly with rsquare. The result suggests that TV advertising budget can explain 64% of the variability in our sales data.

```{r}
rsquare(model1, data = train)
#As a side note, in a simple linear regression model the 
#R2 value will equal the squared correlation between 
#X and Y
cor(train$TV, train$sales)^2
```
Lastly, the F-statistic tests to see if at least one predictor variable has a non-zero coefficient. So, if there is at least a significant independent variable.This becomes more important once we start using multiple predictors as in multiple linear regression; however, we’ll introduce it here. The F-statistic is computed as:
![Equation 8](images/regression/regression8.png)
Hence, a larger F-statistic will produce a statistically significant p-value (p<0.05). In our case we see at the bottom of our summary statement that the F-statistic is 210.8 producing a p-value of p<2.2e−16.
Combined, our RSE, R2, and F-statistic results suggest that our model has an ok fit, but we could likely do better.

### Assessing Our Model Visually
Not only is it important to understand quantitative measures regarding our coefficient and model accuracy but we should also understand visual approaches to assess our model. First, we should always visualize our model within our data when possible. For simple linear regression this is quite simple. Here we use geom_smooth(method = "lm") followed by geom_smooth(). 
This allows us to compare the linearity of our model (blue line with the 95% confidence interval in shaded region) with a non-linear LOESS model. Considering the LOESS smoother remains within the confidence interval we can assume the linear trend fits the essence of this relationship.
However, we do note that as the TV advertising budget gets closer to 0 there is a stronger reduction in sales beyond what the linear trend follows.
```{r}
ggplot(train, aes(TV, sales)) +
  geom_point() +
  geom_smooth(method = "lm") +
  geom_smooth(se = FALSE, color = "red")
```
An important part of assessing regression models is visualizing residuals (important to check regression assumptions and validity of our results). If you use plot(model1) four residual plots will be produced that provide some insights. Here I’ll walk through creating each of these plots within ggplot and explain their insights.

The first plot you want to visualize is the *residuals versus fitted values plot*. T
```{r}

model1_results <- augment(model1, train)# add model diagnostics to our training data

ggplot(model1_results, aes(.fitted, .resid)) +
  geom_ref_line(h = 0) +
  geom_point() +
  geom_smooth(se = FALSE) +
  ggtitle("Residuals vs Fitted")
```
The above plot enables us to analyze two important concerns:
- *Non-linearity*: if a discernible pattern (blue line) exists then this suggests either non-linearity or that other attributes have not been adequately captured. Our plot indicates that the *assumption of linearity* is fair.
- *Heteroskedasticity*: an important assumption of linear regression is that the error terms have a constant variance,
Var(ϵi)=σ2. If there is a funnel shape with our residuals, as slightly in our plot, then we’ve violated the *homoskedasticity assumption*. Sometimes this can be resolved with a log or square root transformation of Y in our model.
```{r}
model1b <- lm(sqrt(sales) ~ TV, data = train)#let's try with applying a square root transformation
model1b_results <- augment(model1b, train)# add model diagnostics to our training data

ggplot(model1b_results, aes(.fitted, .resid)) +
  geom_ref_line(h = 0) +
  geom_point() +
  geom_smooth(se = FALSE) +
  ggtitle("Residuals vs Fitted")#we have reduced the funnel shape and now the observations seems not too have a precise shape which is what we want in this plot
```
```{r}
#Exercise: try to apply a log transfortion and see how the residuals vs fitted plot changes
?log
model1c <- lm(log(sales) ~ TV, data = train)#let's try with applying a square root transformation
model1c_results <- augment(model1c, train)# add model diagnostics to our training data

ggplot(model1c_results, aes(.fitted, .resid)) +
  geom_ref_line(h = 0) +
  geom_point() +
  geom_smooth(se = FALSE) +
  ggtitle("Residuals vs Fitted")
```

We can get this same kind of information with a couple other plots which you will see when using plot(model1). The first is comparing standardized residuals versus fitted values. This is the same plot as above but with the residuals standardized to show where residuals deviate by 1, 2, 3+ standard deviations. This helps us to identify outliers that exceed 3 standard deviations. The second is the scale-location plot. This plot shows if residuals are spread equally along the ranges of predictors. This is how you can check the assumption of equal variance (homoscedasticity). It’s good if you see a horizontal line with equally (randomly) spread points.
```{r}
p1 <- ggplot(model1_results, aes(.fitted, .std.resid)) +
  geom_ref_line(h = 0) +
  geom_point() +
  geom_smooth(se = FALSE) +
  ggtitle("Standardized Residuals vs Fitted")

p2 <- ggplot(model1_results, aes(.fitted, sqrt(.std.resid))) +
  geom_ref_line(h = 0) +
  geom_point() +
  geom_smooth(se = FALSE) +
  ggtitle("Scale-Location")

gridExtra::grid.arrange(p1, p2, nrow = 1)
```

The next plot assess the *normality of the residuals* (third assumption). A Q-Q plot plots the distribution of our residuals against the theoretical normal distribution. The closer the points are to falling directly on the diagonal line then the more we can interpret the residuals as normally distributed. If there is strong snaking or deviations from the diagonal line then we should consider our residuals non-normally distributed. In our case we have a little deviation in the bottom left-hand side which likely is the concern we mentioned earlier that as the TV advertising budget approaches 0 the relationship with sales appears to start veering away from a linear relationship.
```{r}
qq_plot <- qqnorm(model1_results$.resid)
qq_plot <- qqline(model1_results$.resid)
```

Last are the *Cook’s Distance and residuals versus leverage plot*. These plot helps us to find *influential cases* (i.e., subjects) if any. Not all outliers are influential in linear regression analysis. Even though data have extreme values, they might not be influential to determine a regression line. That means, the results wouldn’t be much different if we either include or exclude them from analysis. They follow the trend in the majority of cases and they don’t really matter; they are not influential. On the other hand, some cases could be very influential even if they look to be within a reasonable range of the values. They could be extreme cases against a regression line and can alter the results if we exclude them from analysis. Another way to put it is that they don’t get along with the trend in the majority of the cases.

Here we are looking for outlying values (we can select the top n outliers to report with id.n. The identified (labeled) points represent those dots where cases can be influential against a regression line. When cases have high Cook’s distance scores and are to the upper or lower right of our leverage plot they have leverage meaning they are influential to the regression results. The regression results will be altered if we exclude those cases.
```{r}
par(mfrow=c(1, 2),margin(1,1,1,1))

plot(model1, which = 4, id.n = 5)
plot(model1, which = 5, id.n = 5)#five observations are potentially influential observation
```
If you want to look at these top 5 observations with the highest Cook’s distance in case you want to assess them further you can use the following.
```{r}
model1_results %>%
  top_n(5, wt = .cooksd)
```
So, what does having patterns in residuals mean to your research? It’s not just a go-or-stop sign. It tells you about your model and data. Your current model might not be the best way to understand your data if there’s so much good stuff left in the data.

In that case, you may want to go back to your theory and hypotheses. Is it really a linear relationship between the predictors and the outcome? You may want to include a quadratic term, for example. A log transformation may better represent the phenomena that you’d like to model. Or, is there any important variable that you left out from your model? Other variables you didn’t include (e.g., Radio or Newspaper advertising budgets) may play an important role in your model and data. Or, maybe, your data were systematically biased when collecting data. You may want to redesign data collection methods.

*Checking residuals is a way to discover new insights in your model and data!*

### Making Predictions
Often the goal with regression approaches is to make predictions on new data. To assess how well our model will do in this endeavor we need to assess how it does in making predictions against our test data set. This informs us how well our model generalizes to data outside our training set. We can use our model to predict Sales values for our test data by using add_predictions.
```{r}
(test <- test %>% 
  add_predictions(model1))
```
The primary concern is to assess if the out-of-sample mean squared error (MSE), also known as the mean squared prediction error, is substantially higher than the in-sample mean square error, as this is a sign of deficiency in the model. The MSE is computed as:
![Equation 9](images/regression/regression9.png)

We can easily compare the test sample MSE to the training sample MSE with the following. The difference is not that significant. However, this practice becomes more powerful when you are comparing multiple models (*the model with the lowest test MSE is the preferred model*). For example, if you developed a simple linear model with just the Radio advertising budget as the predictor variable, you could then compare our two different simple linear models and the one that produces the lowest test sample MSE is the preferred model.
```{r}

test %>% 
  add_predictions(model1) %>%
  summarise(MSE = mean((sales - pred)^2))# test MSE

train %>% 
  add_predictions(model1) %>%
  summarise(MSE = mean((sales - pred)^2))# training MSE

```

```{r}
#Exercise: Create the regression model described above (Radio is the only predictor)
set.seed(123)
sample <- sample(c(TRUE, FALSE), nrow(advertising), replace = T, prob = c(0.6,0.4))	
train <- advertising[sample, ]														
test <- advertising[!sample, ]

model2 <- lm(sales ~ radio, data = train)	
 	
summary(model2)										
glance(model2)
tidy(model2)
confint(model2)										
sigma(model2)										
sigma(model2)/mean(train$sales)						
rsquare(model2, data = train)						
cor(train$radio, train$sales)^2	

ggplot(train, aes(radio, sales)) +				
  geom_point() +
  geom_smooth(method = "lm") +				
  geom_smooth(se = FALSE, color = "red")

model2_results <- augment(model2, train)				
ggplot(model1_results, aes(.fitted, .resid)) +			
  geom_ref_line(h = 0) +
  geom_point() +
  geom_smooth(se = FALSE) +
  ggtitle("Residuals vs Fitted")

par(mfrow=c(1,1))
qq_plot <- qqnorm(model2_results$.resid)		
qq_plot <- qqline(model2_results$.resid)

par(mfrow=c(1, 2),margin(1,1,1,1))			
plot(model2, which = 4, id.n = 5)			
plot(model2, which = 5, id.n = 5)

model2_results %>%  top_n(5, wt = .cooksd)	

(test <- test %>%   add_predictions(model2))	

test %>% 													
  add_predictions(model2) %>%
  summarise(MSE = mean((sales - pred)^2))# test MSE
train %>% 
  add_predictions(model2) %>%
  summarise(MSE = mean((sales - pred)^2))# training MSE


#Compare test MSE of model1 vs the model above. Which is the preferred model?
# TV is preferred
```

In the next video we will look at how we can extend a simple linear regression model into a multiple regression.
